import requests
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from datetime import datetime, timezone
from scipy.stats import pearsonr, spearmanr
from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import adfuller

# --- SALIDAS ESTANDARIZADAS ---

OUTDIRS = {"figs": "figs", "irfs": "irfs", "tables": "tables", "models": "models"}
for d in OUTDIRS.values():
    os.makedirs(d, exist_ok=True)
    os.makedirs("snapshots", exist_ok=True)

#   Guarda la figura actual en figs/
def savefig_here(name):
    path = os.path.join(OUTDIRS["figs"], name)
    plt.tight_layout()
    plt.savefig(path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"[saved] {path}")

#   Exporta DataFrame a tables/ en CSV
def save_table(df: pd.DataFrame, name: str, index=True):
    path = os.path.join(OUTDIRS["tables"], name)
    df.to_csv(path, index=index)
    print(f"[saved] {path}")

#   Guarda el resumen del VAR en models/ como texto plano
def save_model_summary(res, name: str):
    os.makedirs(OUTDIRS["models"], exist_ok=True)
    path = os.path.join(OUTDIRS["models"], name)
    summ = res.summary()
    txt = summ.as_text() if hasattr(summ, "as_text") else str(summ)
    with open(path, "w", encoding="utf-8") as f:
        f.write(txt)
    print(f"[saved] {path}")


# --- OBTENER DATOS ---

# URL de Coindesk
url = "https://data-api.coindesk.com/index/cc/v1/historical/days"
api_key = os.getenv("COINDESK_API_KEY")

#   Función para convertir fecha a timestamp UNIX
def date_to_unix(date_str: str) -> int:
    dt = datetime.strptime(date_str, "%Y-%m-%d").replace(tzinfo=timezone.utc)
    return int(dt.timestamp())

#   Función para obtener datos históricos desde Coindesk
def get_crypto_data(instrument, market="cadli", from_date=None, to_date=None):
    if not (from_date and to_date):
        raise ValueError("Debes proporcionar 'from_date' y 'to_date'")

    # Calcular límite de días
    start_dt = datetime.strptime(from_date, "%Y-%m-%d")
    end_dt = datetime.strptime(to_date, "%Y-%m-%d")
    limit = (end_dt - start_dt).days + 1

    params = {
        "market": market,
        "instrument": instrument,
        "limit": limit,
        "aggregate": 1,
        "fill": "true",
        "apply_mapping": "true",
        "response_format": "JSON",
        "to_ts": date_to_unix(to_date),
        "api_key": api_key
    }

    response = requests.get(url, params=params)
    data = response.json()

    # Verificar que haya datos válidos
    if "Data" in data and isinstance(data["Data"], list):
        df = pd.DataFrame(data["Data"])
        df["time"] = pd.to_datetime(df["TIMESTAMP"], unit="s", utc=True)
        return df
    else:
        raise ValueError(f"Error en la API o sin datos: {data}")

# Fechas de análisis
start_date = "2020-05-01"
end_date = "2025-05-01"

# Obtener datos de USDT, DAI y BTCOIN
df_usdt = get_crypto_data("USDT-USD", from_date=start_date, to_date=end_date)
df_dai = get_crypto_data("DAI-USD", from_date=start_date, to_date=end_date)
df_bitcoin = get_crypto_data("BTC-USD", from_date=start_date, to_date=end_date)


# --- VALIDACION DE DATA ---

#   Validar info
def add_valid_flags(df):
    cond_close = df["CLOSE"].notna() & (df["CLOSE"] > 0)
    cond_hilo  = df["HIGH"].notna() & df["LOW"].notna() & (df["HIGH"] > 0) & (df["LOW"] > 0) & (df["LOW"] <= df["HIGH"])
    df["valid_close"] = cond_close
    df["valid_hilo"]  = cond_hilo
    return df

df_usdt = add_valid_flags(df_usdt)
df_dai  = add_valid_flags(df_dai)
df_bitcoin = add_valid_flags(df_bitcoin)

#   Validar Continuidad temporal y duplicados
for name, df in [("USDT", df_usdt), ("DAI", df_dai), ("BTC", df_bitcoin)]:
    df.sort_values("time", inplace=True)
    before = len(df)
    df.drop_duplicates(subset="time", keep="last", inplace=True)
    dups = before - len(df)
    gaps = (df["time"].diff().dropna().dt.total_seconds() != 86400).sum()
    print(f"[{name}] duplicados eliminados: {dups} | posibles huecos (≠ 1 día): {gaps}")


# --- CALCULO DE MEDIDAS ---

# Measure (1): Desviación del valor nominal
df_usdt["Measure1"] = df_usdt["CLOSE"] - 1
df_dai["Measure1"] = df_dai["CLOSE"] - 1

# Measure (2): Volatilidad realizada
def calculate_volatility(df):
    df["Measure2"] = np.sqrt((np.log(df["HIGH"]) - np.log(df["LOW"]))**2 / (4 * np.log(2)))
    return df

df_usdt = calculate_volatility(df_usdt)
df_dai = calculate_volatility(df_dai)
df_bitcoin = calculate_volatility(df_bitcoin)

# Measure (3): Actividad de mercado (número de actualizaciones por día)
df_usdt["Measure3"] = df_usdt["TOTAL_INDEX_UPDATES"]
df_dai["Measure3"] = df_dai["TOTAL_INDEX_UPDATES"]


# --- GENERACION DE GRAFICOS ---

# Gráfico Measure (1)
plt.figure(figsize=(10, 6))
plt.plot(df_usdt["time"], df_usdt["Measure1"], label="USDT")
plt.plot(df_dai["time"], df_dai["Measure1"], label="DAI")
plt.axhline(y=0, color="gray", linestyle="--", linewidth=0.8)
plt.title("Stability Measure (1): Deviation from Nominal Value")
plt.xlabel("Date")
plt.ylabel("Deviation")
plt.legend()
plt.grid(alpha=0.3)
savefig_here("m1_timeseries_usdt_dai.png")

# Gráfico Measure (2)
plt.figure(figsize=(10, 6))
plt.plot(df_usdt["time"], df_usdt["Measure2"], label="USDT")
plt.plot(df_dai["time"], df_dai["Measure2"], label="DAI")
plt.title("Stability Measure (2): Realized Volatility")
plt.xlabel("Date")
plt.ylabel("Volatility")
plt.legend()
plt.grid(alpha=0.3)
savefig_here("m2_timeseries_usdt_dai.png")

# Gráfico de línea temporal
plt.figure(figsize=(10, 6))
plt.plot(df_usdt["time"], df_usdt["Measure3"], label="USDT")
plt.plot(df_dai["time"], df_dai["Measure3"], label="DAI")
plt.title("Measure 3: Total Index Updates per Day")
plt.xlabel("Date")
plt.ylabel("Update Count")
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
savefig_here("m3_timeseries_usdt_dai.png")

# Boxplot comparativo
plt.figure(figsize=(8, 6))
plt.boxplot([df_usdt["Measure3"].dropna(), df_dai["Measure3"].dropna()], labels=["USDT", "DAI"])
plt.title("Measure 3 Distribution: Microstructural Instability")
plt.ylabel("Total Index Updates per Day")
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
savefig_here("m3_boxplot_usdt_vs_dai.png")

plt.figure(figsize=(10, 6))
plt.hist(df_usdt["Measure3"].dropna(), bins=30, alpha=0.6, label='USDT', density=True)
plt.hist(df_dai["Measure3"].dropna(), bins=30, alpha=0.6, label='DAI', density=True)
plt.title("Histogram of Measure 3: Daily Index Update Distribution")
plt.xlabel("Total Index Updates")
plt.ylabel("Density")
plt.legend()
plt.grid(alpha=0.3)
plt.tight_layout()
savefig_here("m3_hist_usdt_dai.png")


# --- GENERAR TABLAS DESCRIPTIVAS ---

# Calcular estadísticas descriptivas con Medida 1, Medida 2 y Medida 3
def generate_descriptive_table(stablecoin_name, df, m1, m2, m3):
    stats = []
    for col in [m1, m2, m3]:
        s = df[col].dropna()
        stats.append({
            "Measure": f"{col}_{stablecoin_name}",
            "Obs": int(s.count()),
            "Mean": float(s.mean()),
            "Std.Dev.": float(s.std()),
            "Min": float(s.min()),
            "Max": float(s.max()),
        })
    return stats


# Generar tablas descriptivas para USDT y DAI
stats_usdt = generate_descriptive_table("USDT", df_usdt, "Measure1", "Measure2", "Measure3")
stats_dai = generate_descriptive_table("DAI", df_dai, "Measure1", "Measure2", "Measure3")

# Generar la tabla 1: Descriptive statistics
descriptive_table = pd.DataFrame(stats_usdt + stats_dai)
print("\nTabla Descriptiva:")
print(descriptive_table)
# Descriptivos
save_table(descriptive_table, "descriptive_stats_m1_m2_m3_usdt_dai.csv", index=False)


#   Funciones para Generar métricas
def _date_at_idx(df, idx):
    try:
        return pd.to_datetime(df.loc[idx, "time"]).date()
    except Exception:
        return None

def stats_m1(df):
    s = df["Measure1"].dropna()
    i_max = s.idxmax(); i_min = s.idxmin()
    return {
        "mean": float(s.mean()),
        "std": float(s.std()),
        "max": float(s.max()), "max_date": _date_at_idx(df, i_max),
        "min": float(s.min()), "min_date": _date_at_idx(df, i_min),
        "days_abs_gt_0.5pct": int((s.abs() > 0.005).sum()),
        "days_abs_gt_1pct": int((s.abs() > 0.01).sum()),
    }

def stats_m2(df):
    s = df["Measure2"].dropna()
    i_max = s.idxmax()
    return {
        "mean": float(s.mean()),
        "std": float(s.std()),
        "p95": float(s.quantile(0.95)),
        "max": float(s.max()), "max_date": _date_at_idx(df, i_max),
    }

def stats_m3(df):
    s = df["TOTAL_INDEX_UPDATES"].dropna()
    i_max = s.idxmax()
    return {
        "median": float(s.median()),
        "mean": float(s.mean()),
        "Q1": float(s.quantile(0.25)),
        "Q3": float(s.quantile(0.75)),
        "p90": float(s.quantile(0.90)),
        "p95": float(s.quantile(0.95)),
        "p99": float(s.quantile(0.99)),
        "max": float(s.max()), "max_date": _date_at_idx(df, i_max),
    }

rows = []
for name, df in [("USDT", df_usdt), ("DAI", df_dai)]:
    rows.append({"asset": name, "metric": "M1", **stats_m1(df)})
    rows.append({"asset": name, "metric": "M2", **stats_m2(df)})
    rows.append({"asset": name, "metric": "M3", **stats_m3(df)})

metrics_7_1 = pd.DataFrame(rows)
save_table(metrics_7_1, "metrics_7_1.csv", index=False)

#   Top-3 episodios por M1
def top_abs_m1(df, k=3):
    s = df["Measure1"].abs().dropna()
    idxs = s.nlargest(k).index
    return pd.DataFrame({
        "date": [pd.to_datetime(df.loc[i, "time"]).date() for i in idxs],
        "M1": [float(df.loc[i, "Measure1"]) for i in idxs],
        "M1_abs": [float(abs(df.loc[i, "Measure1"])) for i in idxs],
    })

tops = []
for name, df in [("USDT", df_usdt), ("DAI", df_dai)]:
    t = top_abs_m1(df, 3)
    t.insert(0, "asset", name)
    tops.append(t)
tops_df = pd.concat(tops, ignore_index=True)
save_table(tops_df, "extremes_m1_top3.csv", index=False)

# Top-3 máximos y mínimos de M2
def extremes_top3(df, label):
    s = df[['time','Measure2']].dropna().copy()
    s = s.sort_values('Measure2')
    mins = s.head(3).assign(asset=label, kind='min')
    maxs = s.tail(3).assign(asset=label, kind='max')
    return pd.concat([mins, maxs], axis=0)

ext_m2 = pd.concat([
    extremes_top3(df_usdt.reset_index(), 'USDT'),
    extremes_top3(df_dai.reset_index(),  'DAI')
], axis=0)

ext_m2.rename(columns={'time':'date', 'Measure2':'M2'}, inplace=True)
ext_m2 = ext_m2[['asset','kind','date','M2']].sort_values(['asset','kind','M2'])
ext_m2.to_csv('tables/extremes_m2_top3.csv', index=False)
print('[saved] tables/extremes_m2_top3.csv')

# Conteos de "días de alta volatilidad" para dos umbrales de M2
t1, t2 = 0.02, 0.05   # ejem: 2% y 5%
def counts_over(df, label, thresholds=(t1,t2)):
    s = df['Measure2'].dropna()
    rows = []
    for th in thresholds:
        rows.append({'asset':label, 'threshold':th, 'days': int((s > th).sum())})
    return pd.DataFrame(rows)

cnt_m2 = pd.concat([
    counts_over(df_usdt, 'USDT'),
    counts_over(df_dai,  'DAI')
], axis=0)

cnt_m2.to_csv('tables/m2_counts_over_thresholds.csv', index=False)
print('[saved] tables/m2_counts_over_thresholds.csv')


# --- GENERAR CORRELACIONES ---

# Correlaciones alineadas por fecha
us = df_usdt.set_index('time')[["Measure1","Measure2","Measure3"]].rename(
        columns={"Measure1":"USDT_Measure1","Measure2":"USDT_Measure2","Measure3":"USDT_Measure3"})
da = df_dai.set_index('time')[["Measure1","Measure2","Measure3"]].rename(
        columns={"Measure1":"DAI_Measure1","Measure2":"DAI_Measure2","Measure3":"DAI_Measure3"})

# Filtrar con días donde existen las 6 medidas
corr_df = us.join(da, how="inner").dropna()

# Calcular la matriz de correlación
correlation_matrix = corr_df.corr()

# Imprimir la matriz de correlación completa
print("\nTabla 3: Matriz de correlación")
print(correlation_matrix)
# Matriz de correlación completa
save_table(correlation_matrix, "correlation_matrix_m1_m2_m3_usdt_dai.csv", index=True)


def corr_with_p(x, y, method="pearson"):
    if method == "pearson":
        r, p = pearsonr(x, y)
    elif method == "spearman":
        r, p = spearmanr(x, y)
    else:
        raise ValueError("method debe ser 'pearson' o 'spearman'")
    return r, p

# Matriz de correlacion M1 — USDT vs DAI (Pearson)
pair_m1 = us[['USDT_Measure1']].join(da[['DAI_Measure1']], how='inner').dropna()
r1, p1 = corr_with_p(pair_m1['USDT_Measure1'], pair_m1['DAI_Measure1'], method="pearson")
print(f"\nM1 (Pearson)  — N={len(pair_m1)}: r={r1:.4f}, p={p1:.4f}")

# Matriz de correlacion M2 — USDT vs DAI (Pearson)
pair_m2 = us[['USDT_Measure2']].join(da[['DAI_Measure2']], how='inner').dropna()
r2, p2 = corr_with_p(pair_m2['USDT_Measure2'], pair_m2['DAI_Measure2'], method="pearson")
print(f"M2 (Pearson)  — N={len(pair_m2)}: r={r2:.4f}, p={p2:.4f}")

# Matriz de correlacion M3 — USDT vs DAI (Pearson + Spearman)
pair_m3 = us[['USDT_Measure3']].join(da[['DAI_Measure3']], how='inner').dropna()
r3p, p3p = corr_with_p(pair_m3['USDT_Measure3'], pair_m3['DAI_Measure3'], method="pearson")
r3s, p3s = corr_with_p(pair_m3['USDT_Measure3'], pair_m3['DAI_Measure3'], method="spearman")
print(f"M3 (Pearson)  — N={len(pair_m3)}: r={r3p:.4f}, p={p3p:.4f}")
print(f"M3 (Spearman) — N={len(pair_m3)}: ρ={r3s:.4f}, p={p3s:.4f}")

pairs_rows = [
    {"pair":"M1 USDT~DAI", "method":"pearson",  "N":len(pair_m1), "r":r1,  "p":p1},
    {"pair":"M2 USDT~DAI", "method":"pearson",  "N":len(pair_m2), "r":r2,  "p":p2},
    {"pair":"M3 USDT~DAI", "method":"pearson",  "N":len(pair_m3), "r":r3p, "p":p3p},
    {"pair":"M3 USDT~DAI", "method":"spearman", "N":len(pair_m3), "r":r3s, "p":p3s},
]
corr_pairs = pd.DataFrame(pairs_rows)
save_table(corr_pairs, "correlations_pairs_with_pvalues.csv", index=False)

# Calcular significancia estadística para cada par de medidas
print("\nSignificancia Estadística:")
for (x, y, label) in [
    ("USDT_Measure1","DAI_Measure1","Measure1"),
    ("USDT_Measure2","DAI_Measure2","Measure2"),
    ("USDT_Measure3","DAI_Measure3","Measure3")
]:
    r, p = pearsonr(corr_df[x], corr_df[y])
    print(f"{label}: Coeficiente = {r:.4f}, p-valor = {p:.4f}")


# --- IMPLEMENTAR EL MODELO VAR ---

# Convertir la columna 'time' a formato datetime en los DataFrames
df_usdt['time'] = pd.to_datetime(df_usdt['time'], utc=True)
df_dai['time'] = pd.to_datetime(df_dai['time'],  utc=True)
df_bitcoin['time'] = pd.to_datetime(df_bitcoin['time'], utc=True)

# Establecer la columna 'time' como índice en los DataFrames
df_usdt.set_index('time', inplace=True)
df_dai.set_index('time', inplace=True)
df_bitcoin.set_index('time', inplace=True)

# Intersecciones por seguridad (mismo día entre los tres activos)
idx_common = df_usdt.index.intersection(df_dai.index).intersection(df_bitcoin.index)
if len(idx_common) == 0:
    raise ValueError("No hay intersección temporal entre USDT, DAI y BTC.")

# Construimos dos paneles separados para maximizar N por medida y evitar borrado conjunto:
#  - VAR(M1): [Bitcoin_Volatility, USDT_Measure1, DAI_Measure1]
#  - VAR(M2): [Bitcoin_Volatility, USDT_Measure2, DAI_Measure2]
# El orden importa para IRFs ortogonales (Cholesky): BTC primero.

var_m1 = pd.concat([
    df_bitcoin.loc[idx_common, ["Measure2"]].rename(columns={"Measure2":"Bitcoin_Volatility"}),
    df_usdt.loc[idx_common,   ["Measure1"]].rename(columns={"Measure1":"USDT_Measure1"}),
    df_dai.loc[idx_common,    ["Measure1"]].rename(columns={"Measure1":"DAI_Measure1"})
], axis=1).dropna()

var_m2 = pd.concat([
    df_bitcoin.loc[idx_common, ["Measure2"]].rename(columns={"Measure2":"Bitcoin_Volatility"}),
    df_usdt.loc[idx_common,    ["Measure2"]].rename(columns={"Measure2":"USDT_Measure2"}),
    df_dai.loc[idx_common,     ["Measure2"]].rename(columns={"Measure2":"DAI_Measure2"})
], axis=1).dropna()

#   Asegurar indice diario y regular 'D'
def enforce_daily_freq(df: pd.DataFrame) -> pd.DataFrame:
    inf = pd.infer_freq(df.index)
    if inf == 'D':
        df = df.copy()
        df.index.freq = 'D'
        return df
    return df.asfreq('D').dropna()

var_m1 = enforce_daily_freq(var_m1)
var_m2 = enforce_daily_freq(var_m2)

print(f"[VAR-M1] obs={len(var_m1)} rango={var_m1.index.min()}→{var_m1.index.max()}")
print(f"[VAR-M2] obs={len(var_m2)} rango={var_m2.index.min()}→{var_m2.index.max()}")

def save_adf_report(df_panel: pd.DataFrame, tag: str):
    rows = []
    for c in df_panel.columns:
        s = df_panel[c].dropna()
        stat, pval, usedlag, nobs, crit, icbest = adfuller(s, autolag='AIC')
        rows.append({
            "panel": tag, "series": c, "stat": stat, "p_value": pval,
            "used_lag": usedlag, "nobs": nobs,
            "crit_1%": crit.get("1%"), "crit_5%": crit.get("5%"), "crit_10%": crit.get("10%"),
            "icbest": icbest
        })
    df_adf = pd.DataFrame(rows)
    save_table(df_adf, f"adf_{tag}.csv", index=False)

# ADF por serie con autolag='AIC'. Si alguna no es estacionaria p>0.05, aplicamos diferencia 1.
def prepare_stationary(df_ordered: pd.DataFrame):
    need_diff = []
    for c in df_ordered.columns:
        # autolag='AIC' mejora potencia en muestras finitas; regresión con constante (por defecto)
        try:
            pval = adfuller(df_ordered[c].dropna(), autolag='AIC')[1]
        except Exception:
            pval = adfuller(df_ordered[c].dropna())[1]
        print(f"ADF {c}: p={pval:.4f}")
        if pval > 0.05:
            need_diff.append(c)
    if need_diff:
        print(f"→ Se diferencia (al menos una serie no estacionaria): {need_diff}")
        return df_ordered.diff().dropna(), "diff"
    else:
        print("→ Todas estacionarias: se trabaja en niveles")
        return df_ordered, "levels"


save_adf_report(var_m1, "M1_raw")
save_adf_report(var_m2, "M2_raw")

var_m1_ready, tr_m1 = prepare_stationary(var_m1)
var_m2_ready, tr_m2 = prepare_stationary(var_m2)

save_adf_report(var_m1_ready, f"M1_{tr_m1}")
save_adf_report(var_m2_ready, f"M2_{tr_m2}")

#   Plotea y guarda una IRF ortogonalizada a disco
def save_irf_plot(irf, fname, title, outdir="irfs"):
    os.makedirs(outdir, exist_ok=True)
    fig = irf.plot(orth=True, figsize=(12, 8))
    plt.suptitle(title)
    plt.tight_layout()
    path = os.path.join(outdir, fname)
    plt.savefig(path, dpi=300, bbox_inches="tight")
    plt.close(fig)
    print(f"[saved] {path}")

# Estimamos SIEMPRE dos VAR:
#   (A) p=5 (comparabilidad con el paper)
#   (B) p seleccionado por AIC (robustez)
# Graficamos IRFs ortogonalizadas(Cholesky) para ambas especificaciones, con el mismo orden de variables.
def fit_var_and_irfs(df_ready: pd.DataFrame,
                     title: str,
                     tag: str,                 # "M1" o "M2"
                     p_paper: int = 5,
                     horizon: int = 10,
                     maxlags: int = 15):

    # Regulariza índice diario y fija freq
    df_ready = enforce_daily_freq(df_ready)

    # Límite de rezago permitido por tamaño muestral
    maxlag_allowed = min(maxlags, len(df_ready) - 1)
    if maxlag_allowed < 1:
        raise ValueError("Panel demasiado corto para VAR: se requieren al menos 2 observaciones.")

    if p_paper > maxlag_allowed:
        print(f"[warn] p_paper={p_paper} > máximo permitido ({maxlag_allowed}); se ajusta a {maxlag_allowed}.")
        p_paper = maxlag_allowed

    # ===== (A) VAR con p del paper =====
    model_paper = VAR(df_ready)
    res_paper = model_paper.fit(p_paper)
    print(f"\n[{title}] VAR principal (paper): p={p_paper}")
    print(res_paper.summary())

    try:
        save_model_summary(res_paper, f"var_{tag}_paper_p{p_paper}.txt")
    except NameError:
        os.makedirs("models", exist_ok=True)
        with open(os.path.join("models", f"var_{tag}_paper_p{p_paper}.txt"), "w", encoding="utf-8") as f:
            f.write(str(res_paper.summary()))

    # IRF del modelo "paper"
    irf_paper = res_paper.irf(horizon)
    save_irf_plot(irf_paper,
                  fname=f"irf_{tag}_paper_p{p_paper}.png",
                  title=f"{title} — IRFs ortogonales (p={p_paper}, especificación paper)")

    # ===== (B) VAR con p seleccionado por AIC =====
    model_aic = VAR(df_ready)
    sel = model_aic.select_order(maxlags=maxlag_allowed)

    # p sugerido (mapea diferencias de versión: dict vs escalar vs secuencia)
    p_aic = None
    try:
        p_aic = sel.selected_orders.get('aic', None)
    except Exception:
        pass

    if p_aic is None:
        aic_attr = getattr(sel, "aic", None)
        if aic_attr is not None:
            if np.isscalar(aic_attr):
                p_aic = int(aic_attr)
            else:
                aic_seq = np.asarray(aic_attr)
                if aic_seq.size > 1:
                    seq = aic_seq[1:].astype(float)
                    p_aic = int(np.nanargmin(seq) + 1)

    if p_aic is None or p_aic < 1:
        p_aic = min(1, maxlag_allowed)
    if p_aic > maxlag_allowed:
        print(f"[info] p_aic sugerido ({p_aic}) > máximo permitido ({maxlag_allowed}); se ajusta a {maxlag_allowed}.")
        p_aic = maxlag_allowed

    print(f"[{title}] Robustez: AIC sugiere p={p_aic}")

    # Persistir tabla de IC por lag si la versión lo expone
    def _as_array(x):
        if x is None or np.isscalar(x):
            return None
        return np.asarray(x)

    def _trim01(arr):
        if arr is None:
            return None
        return arr[1:] if len(arr) > 0 else arr  # quitar lag 0

    AIC = _trim01(_as_array(getattr(sel, "aic",  None)))
    BIC = _trim01(_as_array(getattr(sel, "bic",  None)))
    HQ  = _trim01(_as_array(getattr(sel, "hqic", None)))
    FPE = _trim01(_as_array(getattr(sel, "fpe",  None)))

    if any(v is not None for v in (AIC, BIC, HQ, FPE)):
        any_seq = next(v for v in (AIC, BIC, HQ, FPE) if v is not None)
        ks = np.arange(1, len(any_seq) + 1)
        df_ic = pd.DataFrame({"lag": ks})
        if AIC is not None: df_ic["AIC"] = AIC
        if BIC is not None: df_ic["BIC"] = BIC
        if HQ  is not None: df_ic["HQ"]  = HQ
        if FPE is not None: df_ic["FPE"] = FPE
        try:
            save_table(df_ic, f"lag_selection_{tag}_max{maxlag_allowed}.csv", index=False)
        except NameError:
            os.makedirs("tables", exist_ok=True)
            df_ic.to_csv(os.path.join("tables", f"lag_selection_{tag}_max{maxlag_allowed}.csv"),
                         index=False, encoding="utf-8")

    # Ajuste y persistencia del VAR-AIC
    res_aic = model_aic.fit(p_aic)
    print(res_aic.summary())
    try:
        save_model_summary(res_aic, f"var_{tag}_aic_p{p_aic}.txt")
    except NameError:
        os.makedirs("models", exist_ok=True)
        with open(os.path.join("models", f"var_{tag}_aic_p{p_aic}.txt"), "w", encoding="utf-8") as f:
            f.write(str(res_aic.summary()))

    irf_aic = res_aic.irf(horizon)
    save_irf_plot(irf_aic,
                  fname=f"irf_{tag}_aic_p{p_aic}.png",
                  title=f"{title} — IRFs ortogonales (p=AIC={p_aic}, robustez)")

    return res_paper, res_aic, p_aic



res_m1_paper, res_m1_aic, p_m1_aic = fit_var_and_irfs(var_m1_ready, title=f"VAR — M1 [BTC, USDT, DAI] — {tr_m1}", tag="M1", p_paper=5, horizon=10, maxlags=15)
res_m2_paper, res_m2_aic, p_m2_aic = fit_var_and_irfs(var_m2_ready, title=f"VAR — M2 [BTC, USDT, DAI] — {tr_m2}", tag="M2", p_paper=5, horizon=10, maxlags=15)

save_table(var_m1, "var_panel_M1_raw.csv")
save_table(var_m2, "var_panel_M2_raw.csv")
save_table(var_m1_ready, f"var_panel_M1_{tr_m1}.csv")
save_table(var_m2_ready, f"var_panel_M2_{tr_m2}.csv")


#   Exportar IRFs a CSV
def export_irf(res, horizon, tag):
    irf = res.irf(horizon)
    arr = irf.irfs
    k = arr.shape[1]
    steps = np.arange(arr.shape[0])
    records = []
    cols = list(res.names)
    for h in steps:
        for j in range(k):
            for i in range(k):
                records.append({
                    "step": int(h),
                    "shock": cols[j],
                    "response": cols[i],
                    "irf": float(arr[h, i, j])
                })
    df_irf = pd.DataFrame(records)
    save_table(df_irf, f"irf_values_{tag}.csv", index=False)

# Exporta para M1 (paper y AIC)
export_irf(res_m1_paper, horizon=10, tag=f"M1_paper_p5")
export_irf(res_m1_aic,   horizon=10, tag=f"M1_aic_p{p_m1_aic}")
# Exporta para M2 (paper y AIC)
export_irf(res_m2_paper, horizon=10, tag=f"M2_paper_p5")
export_irf(res_m2_aic,   horizon=10, tag=f"M2_aic_p{p_m2_aic}")


#Snapshot de datos
df_usdt.to_csv("snapshots/df_usdt.csv", index=False)
df_dai.to_csv("snapshots/df_dai.csv",   index=False)
df_bitcoin.to_csv("snapshots/df_bitcoin.csv", index=False)